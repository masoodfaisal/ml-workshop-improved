{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JupyterHub Notebook\n",
    "\n",
    "### This notebook server is hosted on the OpenShift platform which provides a separate server for each individual user. The platform takes care of the provisioning of the server and allocating related to storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, install and import required libraries, watermark our file, initialise our Spark Session Builder and initialise our environment with required configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting watermark\n",
      "  Downloading watermark-2.2.0-py2.py3-none-any.whl (6.8 kB)\n",
      "Requirement already satisfied: ipython in /opt/app-root/lib/python3.8/site-packages (from watermark) (7.22.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/app-root/lib/python3.8/site-packages (from ipython->watermark) (0.17.2)\n",
      "Requirement already satisfied: pygments in /opt/app-root/lib/python3.8/site-packages (from ipython->watermark) (2.8.1)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/app-root/lib/python3.8/site-packages (from ipython->watermark) (3.0.18)\n",
      "Requirement already satisfied: decorator in /opt/app-root/lib/python3.8/site-packages (from ipython->watermark) (5.0.7)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/app-root/lib/python3.8/site-packages (from ipython->watermark) (57.4.0)\n",
      "Requirement already satisfied: pickleshare in /opt/app-root/lib/python3.8/site-packages (from ipython->watermark) (0.7.5)\n",
      "Requirement already satisfied: backcall in /opt/app-root/lib/python3.8/site-packages (from ipython->watermark) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/app-root/lib/python3.8/site-packages (from ipython->watermark) (5.0.5)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/app-root/lib/python3.8/site-packages (from ipython->watermark) (4.8.0)\n",
      "Requirement already satisfied: parso<0.8.0,>=0.7.0 in /opt/app-root/lib/python3.8/site-packages (from jedi>=0.16->ipython->watermark) (0.7.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/app-root/lib/python3.8/site-packages (from pexpect>4.3->ipython->watermark) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/app-root/lib/python3.8/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->watermark) (0.2.5)\n",
      "Requirement already satisfied: ipython-genutils in /opt/app-root/lib/python3.8/site-packages (from traitlets>=4.2->ipython->watermark) (0.2.0)\n",
      "Installing collected packages: watermark\n",
      "Successfully installed watermark-2.2.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/opt/app-root/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: Minio in /opt/app-root/lib/python3.8/site-packages (6.0.2)\n",
      "Requirement already satisfied: urllib3 in /opt/app-root/lib/python3.8/site-packages (from Minio) (1.26.4)\n",
      "Requirement already satisfied: pytz in /opt/app-root/lib/python3.8/site-packages (from Minio) (2021.1)\n",
      "Requirement already satisfied: python-dateutil in /opt/app-root/lib/python3.8/site-packages (from Minio) (2.8.1)\n",
      "Requirement already satisfied: certifi in /opt/app-root/lib/python3.8/site-packages (from Minio) (2020.12.5)\n",
      "Requirement already satisfied: configparser in /opt/app-root/lib/python3.8/site-packages (from Minio) (5.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.8/site-packages (from python-dateutil->Minio) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/opt/app-root/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.5.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.3 MB 26.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/app-root/lib/python3.8/site-packages (from matplotlib) (20.9)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.3.2-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 118.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting setuptools-scm>=4\n",
      "  Downloading setuptools_scm-6.3.2-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/app-root/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/app-root/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Collecting numpy>=1.17\n",
      "  Downloading numpy-1.21.4-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.7 MB 116.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pillow>=6.2.0\n",
      "  Downloading Pillow-8.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 112.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.28.2-py3-none-any.whl (880 kB)\n",
      "\u001b[K     |████████████████████████████████| 880 kB 112.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /opt/app-root/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: setuptools in /opt/app-root/lib/python3.8/site-packages (from setuptools-scm>=4->matplotlib) (57.4.0)\n",
      "Collecting tomli>=1.0.0\n",
      "  Downloading tomli-1.2.2-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: tomli, setuptools-scm, pillow, numpy, kiwisolver, fonttools, cycler, matplotlib\n",
      "Successfully installed cycler-0.11.0 fonttools-4.28.2 kiwisolver-1.3.2 matplotlib-3.5.0 numpy-1.21.4 pillow-8.4.0 setuptools-scm-6.3.2 tomli-1.2.2\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/opt/app-root/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install watermark\n",
    "%pip install Minio\n",
    "%pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import from_json, col, to_json, struct\n",
    "import watermark\n",
    "from minio import Minio\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.8.6\n",
      "IPython version      : 7.22.0\n",
      "\n",
      "Compiler    : GCC 8.4.1 20200928 (Red Hat 8.4.1-1)\n",
      "OS          : Linux\n",
      "Release     : 4.18.0-240.22.1.el8_3.x86_64\n",
      "Machine     : x86_64\n",
      "Processor   : x86_64\n",
      "CPU cores   : 32\n",
      "Architecture: 64bit\n",
      "\n",
      "Git hash: d93a18cffaf35f2871b99f5a3dc16990dfb92072\n",
      "\n",
      "json     : 2.0.9\n",
      "watermark: 2.2.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -n -v -m -g -iv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyper_parameters import get_hyper_paras\n",
    "user_id,PROJECT_NAME,EXPERIMENT_NAME,experiment_name = get_hyper_paras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sparkSessionBuilder = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Customer Churn ingest Pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "submit_args = \"--conf spark.jars.ivy=/tmp \\\n",
    "--conf spark.hadoop.fs.s3a.endpoint=http://minio-ml-workshop:9000 \\\n",
    "--conf spark.hadoop.fs.s3a.access.key=minio \\\n",
    "--conf spark.hadoop.fs.s3a.secret.key=minio123 \\\n",
    "--conf spark.hadoop.fs.s3a.path.style.access=true \\\n",
    "--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \\\n",
    "--packages org.apache.hadoop:hadoop-aws:3.2.0\"\n",
    "\n",
    "\n",
    "# submit_args = \"--conf spark.jars.ivy=/tmp \\\n",
    "# --conf spark.hadoop.fs.s3a.endpoint=http://minio-ml-workshop:9000 \\\n",
    "# --conf spark.hadoop.fs.s3a.access.key=minio \\\n",
    "# --conf spark.hadoop.fs.s3a.secret.key=minio123 \\\n",
    "# --conf spark.hadoop.fs.s3a.path.style.access=true \\\n",
    "# --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \\\n",
    "# --packages org.apache.hadoop:hadoop-aws:3.2.0,\\\n",
    "# org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1,\\\n",
    "# org.apache.kafka:kafka-clients:2.8.0,\\\n",
    "# org.apache.spark:spark-streaming_2.12:3.0.1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Connect to Spark Cluster provided by OpenShift Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing environment variables for Spark\n",
      "Creating a spark session...\n",
      "Spark session created\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <br/>\n",
       "            <p><b>Spark Context</b></p>\n",
       "            <dl>\n",
       "              <dt>Cluster Name</dt>\n",
       "                <dd><code>spark-cluster-fmasood</code></dd>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-cluster-fmasood:7077</code></dd>\n",
       "              <dt>App Id</dt>\n",
       "                <dd><code>app-20211202013832-0000</code></dd>\n",
       "              <dt>App Name</dt>\n",
       "                <dd><code>ML Ops Demo</code></dd>\n",
       "              <dt>Driver IP</dt>\n",
       "                <dd><code>10.130.2.13</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark context started.\n"
     ]
    }
   ],
   "source": [
    "import spark_util\n",
    "\n",
    "spark = spark_util.getOrCreateSparkSession(\"ML Ops Demo\", submit_args)\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "print('Spark context started.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Declare our input data sources, import and combine them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerID: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- SeniorCitizen: integer (nullable = true)\n",
      " |-- Partner: string (nullable = true)\n",
      " |-- Dependents: string (nullable = true)\n",
      " |-- tenure: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrame_Customer = spark.read\\\n",
    "                .options(delimeter=',', inferSchema='True', header='True') \\\n",
    "                .csv(\"s3a://rawdata/customers/Customer-Churn_P1.csv\")\n",
    "dataFrame_Customer.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customerID: integer (nullable = true)\n",
      " |-- PhoneService: string (nullable = true)\n",
      " |-- MultipleLines: string (nullable = true)\n",
      " |-- InternetService: string (nullable = true)\n",
      " |-- OnlineSecurity: string (nullable = true)\n",
      " |-- OnlineBackup: string (nullable = true)\n",
      " |-- DeviceProtection: string (nullable = true)\n",
      " |-- TechSupport: string (nullable = true)\n",
      " |-- StreamingTV: string (nullable = true)\n",
      " |-- StreamingMovies: string (nullable = true)\n",
      " |-- Contract: string (nullable = true)\n",
      " |-- PaperlessBilling: string (nullable = true)\n",
      " |-- PaymentMethod: string (nullable = true)\n",
      " |-- MonthlyCharges: double (nullable = true)\n",
      " |-- TotalCharges: double (nullable = true)\n",
      " |-- Churn: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrame_Products = spark.read\\\n",
    "                .options(delimeter=',', inferSchema='True', header='True') \\\n",
    "                .csv(\"s3a://rawdata/products/Customer-Churn_P2.csv\")\n",
    "dataFrame_Products.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import *\n",
    "# from  pyspark.sql.functions import *\n",
    "\n",
    "# srcKafkaBrokers = \"odh-message-bus-kafka-bootstrap:9092\"\n",
    "# srcKakaTopic = \"datatelco\"\n",
    "\n",
    "\n",
    "\n",
    "# schema = StructType()\\\n",
    "#     .add(\"customerID\", IntegerType())\\\n",
    "#     .add(\"PhoneService\", StringType())\\\n",
    "#     .add(\"MultipleLines\", StringType())\\\n",
    "#     .add(\"InternetService\", StringType())\\\n",
    "#     .add(\"OnlineSecurity\", StringType())\\\n",
    "#     .add(\"OnlineBackup\", StringType())\\\n",
    "#     .add(\"DeviceProtection\", StringType())\\\n",
    "#     .add(\"TechSupport\", StringType())\\\n",
    "#     .add(\"StreamingTV\", StringType())\\\n",
    "#     .add(\"StreamingMovies\", StringType())\\\n",
    "#     .add(\"Contract\", StringType())\\\n",
    "#     .add(\"PaperlessBilling\", StringType())\\\n",
    "#     .add(\"PaymentMethod\", StringType())\\\n",
    "#     .add(\"MonthlyCharges\", StringType())\\\n",
    "#     .add(\"TotalCharges\", DoubleType())\\\n",
    "#     .add(\"Churn\", StringType())\n",
    "\n",
    "\n",
    "\n",
    "# #Read from JSON Kafka messages into a dataframe\n",
    "# dfKafka = spark.read.format(\"kafka\")\\\n",
    "#     .option(\"kafka.bootstrap.servers\", srcKafkaBrokers)\\\n",
    "#     .option(\"subscribe\", srcKakaTopic)\\\n",
    "#     .option(\"startingOffsets\", \"earliest\")\\\n",
    "#     .load()\\\n",
    "#     .withColumn(\"value\", regexp_replace(col(\"value\").cast(\"string\"), \"\\\\\\\\\", \"\")) \\\n",
    "#     .withColumn(\"value\", regexp_replace(col(\"value\"), \"^\\\"|\\\"$\", \"\")) \\\n",
    "#     .selectExpr(\"CAST(value AS STRING) as jsonValue\")\\\n",
    "#     .rdd.map(lambda row: row[\"jsonValue\"])\n",
    "\n",
    "# dfObj = spark.read.schema(schema).json(dfKafka)\n",
    "# dfObj.printSchema()\n",
    "# dfObj.show(n=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataFrom_All = dataFrame_Customer.join(dataFrame_Products, \"customerID\", how=\"full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Push prepared data to object storage and stop Spark cluster to save resources\n",
    "###  Note - be sure to change this user_id on the next line to your username (something in the range user1 ... user30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# user_id = \"user29\"\n",
    "file_location = \"s3a://data/full_data_csv\" + user_id\n",
    "dataFrom_All.repartition(1).write.mode(\"overwrite\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .format(\"csv\").save(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "545e036c4b32438aced1f6b3c8d38ca151d9c36189e05839cb0aa568fda70ddd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
