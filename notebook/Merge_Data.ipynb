{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JupyterHub Notebook\n",
    "\n",
    "### This notebook server is hosted on the OpenShift platform which provides a separate server for each individual user. The platform takes care of the provisioning of the server and allocating related to storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, install and import required libraries, watermark our file, initialise our Spark Session Builder and initialise our environment with required configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%pip install watermark\n",
    "%pip install Minio\n",
    "%pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import from_json, col, to_json, struct\n",
    "import watermark\n",
    "from minio import Minio\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%watermark -n -v -m -g -iv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyper_parameters import get_hyper_paras\n",
    "user_id,PROJECT_NAME,EXPERIMENT_NAME,experiment_name, s3BucketFullPath = get_hyper_paras()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sparkSessionBuilder = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"Customer Churn ingest Pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "submit_args = \"--conf spark.jars.ivy=/tmp \\\n",
    "--conf spark.hadoop.fs.s3a.endpoint=http://minio-ml-workshop:9000 \\\n",
    "--conf spark.hadoop.fs.s3a.access.key=minio \\\n",
    "--conf spark.hadoop.fs.s3a.secret.key=minio123 \\\n",
    "--conf spark.hadoop.fs.s3a.path.style.access=true \\\n",
    "--conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \\\n",
    "--packages org.apache.hadoop:hadoop-aws:3.2.0\"\n",
    "\n",
    "\n",
    "# submit_args = \"--conf spark.jars.ivy=/tmp \\\n",
    "# --conf spark.hadoop.fs.s3a.endpoint=http://minio-ml-workshop:9000 \\\n",
    "# --conf spark.hadoop.fs.s3a.access.key=minio \\\n",
    "# --conf spark.hadoop.fs.s3a.secret.key=minio123 \\\n",
    "# --conf spark.hadoop.fs.s3a.path.style.access=true \\\n",
    "# --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \\\n",
    "# --packages org.apache.hadoop:hadoop-aws:3.2.0,\\\n",
    "# org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1,\\\n",
    "# org.apache.kafka:kafka-clients:2.8.0,\\\n",
    "# org.apache.spark:spark-streaming_2.12:3.0.1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Connect to Spark Cluster provided by OpenShift Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import spark_util\n",
    "\n",
    "spark = spark_util.getOrCreateSparkSession(\"ML Ops Demo\", submit_args)\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "print('Spark context started.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Declare our input data sources, import and combine them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataFrame_Customer = spark.read\\\n",
    "                .options(delimeter=',', inferSchema='True', header='True') \\\n",
    "                .csv(\"s3a://rawdata/customers/Customer-Churn_P1.csv\")\n",
    "dataFrame_Customer.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataFrame_Products = spark.read\\\n",
    "                .options(delimeter=',', inferSchema='True', header='True') \\\n",
    "                .csv(\"s3a://rawdata/products/Customer-Churn_P2.csv\")\n",
    "dataFrame_Products.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import *\n",
    "# from  pyspark.sql.functions import *\n",
    "\n",
    "# srcKafkaBrokers = \"odh-message-bus-kafka-bootstrap:9092\"\n",
    "# srcKakaTopic = \"datatelco\"\n",
    "\n",
    "\n",
    "\n",
    "# schema = StructType()\\\n",
    "#     .add(\"customerID\", IntegerType())\\\n",
    "#     .add(\"PhoneService\", StringType())\\\n",
    "#     .add(\"MultipleLines\", StringType())\\\n",
    "#     .add(\"InternetService\", StringType())\\\n",
    "#     .add(\"OnlineSecurity\", StringType())\\\n",
    "#     .add(\"OnlineBackup\", StringType())\\\n",
    "#     .add(\"DeviceProtection\", StringType())\\\n",
    "#     .add(\"TechSupport\", StringType())\\\n",
    "#     .add(\"StreamingTV\", StringType())\\\n",
    "#     .add(\"StreamingMovies\", StringType())\\\n",
    "#     .add(\"Contract\", StringType())\\\n",
    "#     .add(\"PaperlessBilling\", StringType())\\\n",
    "#     .add(\"PaymentMethod\", StringType())\\\n",
    "#     .add(\"MonthlyCharges\", StringType())\\\n",
    "#     .add(\"TotalCharges\", DoubleType())\\\n",
    "#     .add(\"Churn\", StringType())\n",
    "\n",
    "\n",
    "\n",
    "# #Read from JSON Kafka messages into a dataframe\n",
    "# dfKafka = spark.read.format(\"kafka\")\\\n",
    "#     .option(\"kafka.bootstrap.servers\", srcKafkaBrokers)\\\n",
    "#     .option(\"subscribe\", srcKakaTopic)\\\n",
    "#     .option(\"startingOffsets\", \"earliest\")\\\n",
    "#     .load()\\\n",
    "#     .withColumn(\"value\", regexp_replace(col(\"value\").cast(\"string\"), \"\\\\\\\\\", \"\")) \\\n",
    "#     .withColumn(\"value\", regexp_replace(col(\"value\"), \"^\\\"|\\\"$\", \"\")) \\\n",
    "#     .selectExpr(\"CAST(value AS STRING) as jsonValue\")\\\n",
    "#     .rdd.map(lambda row: row[\"jsonValue\"])\n",
    "\n",
    "# dfObj = spark.read.schema(schema).json(dfKafka)\n",
    "# dfObj.printSchema()\n",
    "# dfObj.show(n=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataFrom_All = dataFrame_Customer.join(dataFrame_Products, \"customerID\", how=\"full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Push prepared data to object storage and stop Spark cluster to save resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "file_location = \"s3a://data/full_data_csv-\" + user_id\n",
    "dataFrom_All.repartition(1).write.mode(\"overwrite\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .format(\"csv\").save(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Notebook complete!')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "545e036c4b32438aced1f6b3c8d38ca151d9c36189e05839cb0aa568fda70ddd"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
